\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}

\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  urlcolor=blue!60!black
}

\title{\textbf{Z-GED Quick Read Guide}\\
\large What the Code Does, in Plain Language}
\author{Student + Faculty Friendly Summary}
\date{\today}

\begin{document}
\maketitle

\section*{1) The Big Picture (30-second version)}
This project learns to generate circuit topologies (which nodes connect to which, and with what component types) from a compact 8-number latent code.

The flow is:
\begin{enumerate}
  \item Turn each circuit into graph numbers (nodes, edges, poles/zeros).
  \item Encode that graph into 8 latent numbers with a VAE encoder.
  \item Decode those 8 numbers back into a circuit topology.
  \item Train with losses that reward correct topology and connectivity.
\end{enumerate}

\section*{2) What Inputs Look Like in Code}
\textbf{Node features (4 numbers per node)}
\[
[\text{is\_GND},\ \text{is\_VIN},\ \text{is\_VOUT},\ \text{is\_INTERNAL}]
\]
Example: VIN node is \([0,1,0,0]\).

\textbf{Edge features (3 numbers per edge)}
\[
[\log_{10}(R),\ \log_{10}(C),\ \log_{10}(L)]
\]
If a component is absent on that edge, its slot is set to 0.

Example edge with R=1k\(\Omega\), C=100nF, and no inductor:
\[
[3.0,\ -7.0,\ 0.0]
\]
because \(\log_{10}(1000)=3\), \(\log_{10}(10^{-7})=-7\).

\textbf{Poles and zeros}
These are variable-length lists of complex values stored as \([\text{real},\text{imag}]\) pairs.

\section*{3) Encoder: How the Model Compresses a Circuit into 8 Numbers}
Read this in the exact order the encoder runs in code.

\subsection*{Step E1: Start from node and edge tensors}
\begin{itemize}
  \item Nodes: one-hot vectors (size 4 each).
  \item Edges: \([\log_{10}(R), \log_{10}(C), \log_{10}(L)]\) (size 3 each).
\end{itemize}

\subsection*{Step E2: Run 3 GNN layers (ImpedanceGNN)}
Each layer applies an \texttt{ImpedanceConv}, then normalization/activation/residual in the wrapper.

\textbf{Inside one ImpedanceConv layer, per directed edge \(j \to i\):}
\begin{enumerate}
  \item Read edge values:
  \[
  v_R=\log_{10}(R),\quad v_C=\log_{10}(C),\quad v_L=\log_{10}(L).
  \]
  \item Build component masks:
  \[
  m_R=\mathbf{1}(|v_R|>0.01),\quad
  m_C=\mathbf{1}(|v_C|>0.01),\quad
  m_L=\mathbf{1}(|v_L|>0.01).
  \]
  \item Compute component-specific messages:
  \[
  \text{msg}_R=f_R([h_j, v_R]),\quad
  \text{msg}_C=f_C([h_j, v_C]),\quad
  \text{msg}_L=f_L([h_j, v_L]).
  \]
  \item Combine by masks (this is the key update you asked about):
  \[
  \text{msg}_{j\to i}=m_R\,\text{msg}_R + m_C\,\text{msg}_C + m_L\,\text{msg}_L.
  \]
  \item Compute attention weight \(a_{ij}\) from \([h_i,h_j]\), then scale message:
  \[
  \tilde{\text{msg}}_{j\to i}=a_{ij}\,\text{msg}_{j\to i}.
  \]
\end{enumerate}

\textbf{Then per node \(i\):}
\begin{enumerate}
  \item Aggregate incoming messages (sum over neighbors).
  \item Add bias and dropout (inside \texttt{ImpedanceConv}).
  \item In \texttt{ImpedanceGNN}: LayerNorm, ReLU (except last layer), and residual add.
\end{enumerate}

So each layer produces a new node state:
\[
h_i^{(\ell+1)} = \text{ResidualProj}\!\left(h_i^{(\ell)}\right) + \text{PostProcess}\!\left(\sum_{j\in\mathcal{N}(i)} \tilde{\text{msg}}_{j\to i}\right).
\]

\subsection*{Step E3: Build 3 latent branches from final node states}
\begin{itemize}
  \item \textbf{Topology branch (2D):}
  mean+max graph pooling \(\to\) MLP \(\to (\mu_{\text{topo}}, \log\sigma^2_{\text{topo}})\).
  \item \textbf{Structure branch (2D):}
  extract final embeddings of GND, VIN, VOUT, concatenate, MLP
  \(\to (\mu_{\text{struct}}, \log\sigma^2_{\text{struct}})\).
  \item \textbf{Pole-zero branch (4D):}
  DeepSets(poles), DeepSets(zeros), concatenate, MLP
  \(\to (\mu_{\text{pz}}, \log\sigma^2_{\text{pz}})\).
\end{itemize}

\subsection*{Step E4: Concatenate and sample latent code}
\[
\mu=[\mu_{\text{topo}}|\mu_{\text{struct}}|\mu_{\text{pz}}],\quad
\log\sigma^2=[\log\sigma^2_{\text{topo}}|\log\sigma^2_{\text{struct}}|\log\sigma^2_{\text{pz}}].
\]
\[
z=[z_{\text{topology}}(2)\ |\ z_{\text{structure}}(2)\ |\ z_{\text{pz}}(4)] \in \mathbb{R}^8.
\]
\[
z = \mu + \sigma \odot \epsilon,\quad \epsilon \sim \mathcal{N}(0,I).
\]

\section*{4) Decoder: How 8 Numbers Become a Circuit}
Read this section as the exact generation order in code:

\subsection*{Step 1: Build global decoder context from latent \(z\)}
\begin{itemize}
  \item Input is one 8D latent vector.
  \item A context MLP turns \(z\) into a hidden-state vector used by later steps.
\end{itemize}

\subsection*{Step 2: Predict node count first}
\begin{itemize}
  \item A node-count head predicts how many nodes to generate (from 3 to max\_nodes).
  \item This happens before any node/edge sequence decoding.
\end{itemize}

\subsection*{Step 3: Node Transformer runs next (one node at a time)}
For node position \(i\):
\begin{enumerate}
  \item Build current node token from: latent context + position embedding + total-length embedding.
  \item Attend to previously generated node embeddings \([0, \dots, i-1]\).
  \item Predict node-type logits for position \(i\).
  \item Convert predicted type to node embedding and append to history.
\end{enumerate}

\textbf{Node Transformer K/Q/V}
\begin{itemize}
  \item \(Q\): from the current node token (``what do I need now?'').
  \item \(K\): from previous-node memory (``what past info is available?'').
  \item \(V\): from previous-node memory (``what content do I read?'').
\end{itemize}

\subsection*{Step 4: Edge Transformer runs after nodes (one edge-pair step at a time)}
Edge pairs are ordered as \((i,j)\) with \(j < i\). For edge step \(t\):
\begin{enumerate}
  \item Build edge-step token from:
  \begin{itemize}
    \item node-pair embedding (from node \(i,j\)),
    \item latent projection,
    \item edge position embedding,
    \item previous-edge-token embedding.
  \end{itemize}
  \item Run Transformer self-attention with a causal mask.
  \item Predict 8-class edge/component logits for this step.
  \item Feed predicted edge class to later edge steps.
\end{enumerate}

\textbf{Edge Transformer K/Q/V}
\begin{itemize}
  \item \(Q_t\): from current edge-step token \(x_t\).
  \item \(K_{1..t}, V_{1..t}\): from current and earlier edge-step tokens.
  \item Causal mask blocks future steps, so step \(t\) cannot see \(t+1, t+2,\dots\).
\end{itemize}

Edge class is an 8-way choice:
\begin{center}
\begin{tabular}{cl}
\toprule
Class & Meaning \\
\midrule
0 & No edge \\
1 & R \\
2 & C \\
3 & L \\
4 & RC \\
5 & RL \\
6 & CL \\
7 & RCL \\
\bottomrule
\end{tabular}
\end{center}

\subsection*{Step 5: Training vs inference}
\begin{itemize}
  \item Training (teacher forcing): previous edge tokens come from ground truth.
  \item Inference (generation): previous edge tokens come from model predictions.
\end{itemize}

\section*{5) How Numbers Are Manipulated During Training}
At each batch:
\begin{enumerate}
  \item Encoder outputs \(z,\mu,\log\sigma^2\).
  \item Targets are converted to dense matrices (node types, edge existence, component classes).
  \item Decoder predicts logits (unnormalized scores) for:
  \begin{itemize}
    \item node type,
    \item node count,
    \item edge-component class.
  \end{itemize}
  \item Losses compare predicted logits to target labels.
\end{enumerate}

Total loss in code:
\[
\mathcal{L} =
1.0\,\mathcal{L}_{\text{node-type}} +
5.0\,\mathcal{L}_{\text{node-count}} +
2.0\,\mathcal{L}_{\text{edge-comp}} +
5.0\,\mathcal{L}_{\text{connectivity}} +
0.01\,\mathcal{L}_{\text{KL}}.
\]

Interpretation:
\begin{itemize}
  \item Bigger weight = optimizer cares more about that objective.
  \item Connectivity loss strongly pushes VIN/VOUT to remain connected.
  \item KL loss keeps latent space smooth for sampling/interpolation.
\end{itemize}

\section*{6) A Tiny End-to-End Example}
Suppose one circuit edge is represented as:
\[
[3.0,\ -7.0,\ 0.0].
\]
The model interprets this as:
\begin{itemize}
  \item resistor present (1k\(\Omega\)),
  \item capacitor present (100nF),
  \item inductor absent.
\end{itemize}

After full encoding, assume latent code:
\[
z=[1.2,-0.8,\ 0.5,-1.1,\ 0.2,0.1,-0.3,0.4].
\]
Decoder then predicts:
\begin{itemize}
  \item node count (for example 4 nodes),
  \item node sequence (GND, VIN, VOUT, INTERNAL),
  \item each edge class (for example R, C, no-edge, ...).
\end{itemize}

\section*{7) Why This Design Is Easy to Explain}
\begin{itemize}
  \item Graph encoder captures local circuit structure.
  \item Latent code compresses that structure into 8 interpretable dimensions.
  \item Autoregressive decoder builds a valid graph step-by-step.
  \item Joint edge-component class avoids inconsistent edge/type predictions.
\end{itemize}

\section*{8) Current Practical Numbers}
Using current production code:
\begin{itemize}
  \item Encoder parameters: 83,411
  \item Decoder parameters: 7,698,901
  \item Dataset size: 360 circuits (288 train / 72 validation)
\end{itemize}

\section*{9) Where to Look in the Repository}
\begin{itemize}
  \item \path{ml/data/dataset.py}
  \item \path{ml/models/gnn_layers.py}
  \item \path{ml/models/encoder.py}
  \item \path{ml/models/decoder.py}
  \item \path{ml/losses/circuit_loss.py}
  \item \path{scripts/training/train.py}
\end{itemize}

\section*{One-line takeaway}
This code learns a compact 8-number representation of circuits, then uses that representation to generate circuit topology step-by-step with graph-aware deep learning.

\end{document}
