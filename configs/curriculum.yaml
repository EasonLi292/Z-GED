# GraphVAE Configuration with Curriculum Learning
# Enables topology weight curriculum: starts high (3x), anneals to 1x over 20 epochs

# Data configuration
data:
  dataset_path: "rlc_dataset/filter_dataset.pkl"
  normalize: true
  log_scale: true
  train_ratio: 0.8
  val_ratio: 0.1
  split_seed: 42

# Model architecture
model:
  # Node and edge features
  node_feature_dim: 4  # [GND, VIN, VOUT, INTERNAL]
  edge_feature_dim: 7  # [log(C), log(G), log(L_inv), has_C, has_R, has_L, is_parallel]

  # GNN encoder
  gnn_hidden_dim: 64
  gnn_num_layers: 3

  # Latent space
  latent_dim: 24  # Split into 3 Ã— 8D [topo, values, pz]

  # Decoder
  decoder_hidden_dim: 128

  # Regularization
  dropout: 0.1

# Loss function (with curriculum learning)
loss:
  recon_weight: 1.0
  tf_weight: 0.01     # Low TF weight (Chamfer distance is large)
  kl_weight: 0.1      # Increased KL for better regularization

  # Curriculum learning for topology weight
  use_topo_curriculum: true
  topo_curriculum_warmup_epochs: 20
  topo_curriculum_initial_multiplier: 3.0  # Start at 3x, anneal to 1x

# Training configuration
training:
  # Optimization
  optimizer: "adamw"
  learning_rate: 5.0e-4
  weight_decay: 1.0e-5

  # Learning rate scheduling
  scheduler: "cosine"
  min_lr: 1.0e-6
  T_0: 50
  T_mult: 2

  # Training parameters
  epochs: 200
  batch_size: 4

  # Validation and logging
  val_interval: 1
  log_interval: 5

  # Early stopping
  early_stopping_patience: 30

  # Checkpointing
  checkpoint_dir: "checkpoints"

# Regularization
regularization:
  max_grad_norm: 1.0
  spectral_norm: false

# Hardware
hardware:
  num_workers: 0
  pin_memory: false
