# 8D Model Configuration with Improved Transfer Function Prediction
#
# Changes from baseline (8d_conservative.yaml):
# - Increased tf_weight: 0.01 → 1.0 (100x increase)
# - Reduced topo curriculum multiplier: 3.0 → 2.0 (balance topology vs transfer function)
# - Added tf curriculum: gradually increase tf_weight over training
#
# Rationale:
# - Current model has 0% transfer function inference accuracy
# - Predicted poles/zeros don't match circuit behavior
# - Strong supervision needed for auxiliary prediction task
#
# Expected improvements:
# - Better pole/zero predictions
# - Transfer function inference: 0% → 60%+ accuracy
# - Maintained topology accuracy: 100%

data:
  dataset_path: rlc_dataset/filter_dataset.pkl
  train_ratio: 0.8
  val_ratio: 0.1
  split_seed: 42
  normalize: true
  log_scale: true

model:
  # Architecture (same as 8D model)
  latent_dim: 8
  topo_latent_dim: 2    # Topology branch
  values_latent_dim: 2  # Component values branch
  pz_latent_dim: 4      # Poles/zeros branch (increased capacity)

  # GNN encoder
  node_feature_dim: 4
  edge_feature_dim: 7
  gnn_hidden_dim: 64
  gnn_num_layers: 3

  # Decoder
  decoder_hidden_dim: 128
  dropout: 0.1

loss:
  # Reconstruction loss
  recon_weight: 1.0

  # Transfer function loss - INCREASED with curriculum
  tf_weight: 1.0  # Target/final weight (was 0.01)
  use_tf_curriculum: true  # Enable TF curriculum
  tf_curriculum_warmup_epochs: 50  # Gradually increase over 50 epochs
  tf_curriculum_initial_multiplier: 0.01  # Start at 1% of target (0.01)

  # Topology loss with curriculum
  use_topo_curriculum: true
  topo_curriculum_warmup_epochs: 30  # Extended from 20
  topo_curriculum_initial_multiplier: 2.0  # Reduced from 3.0

  # KL divergence
  kl_weight: 0.1

  # GED metric learning (disabled for now)
  use_ged_loss: false
  ged_weight: 0.5
  ged_matrix_path: rlc_dataset/ged_matrix.npy

training:
  # Optimizer
  optimizer: adamw
  learning_rate: 0.0005
  weight_decay: 0.00001

  # Scheduler - Cosine annealing with warm restarts
  scheduler: cosine
  T_0: 50      # First restart at epoch 50
  T_mult: 2    # Double period each restart
  min_lr: 0.000001

  # Training settings
  epochs: 200
  batch_size: 4
  checkpoint_dir: checkpoints

  # Teacher forcing (conditional generation)
  use_teacher_forcing: true

  # Validation
  val_interval: 1
  log_interval: 5
  early_stopping_patience: 30

regularization:
  max_grad_norm: 1.0
  spectral_norm: false

hardware:
  num_workers: 0
  pin_memory: false
