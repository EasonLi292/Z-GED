# Configuration for GraphGPT Autoregressive Circuit Decoder Training

# Model Architecture
model:
  # Encoder (will be loaded from pretrained checkpoint)
  encoder:
    node_feature_dim: 4
    edge_feature_dim: 7
    gnn_hidden_dim: 64
    gnn_num_layers: 3
    latent_dim: 8
    topo_latent_dim: 2
    values_latent_dim: 2
    pz_latent_dim: 4
    dropout: 0.1

  # GraphGPT Decoder
  decoder:
    latent_dim: 8                # Latent code dimension from encoder
    conditions_dim: 2            # Specifications dimension (cutoff, Q)
    hidden_dim: 256              # Hidden dimension for transformer
    num_heads: 8                 # Number of attention heads
    num_node_layers: 4           # Number of transformer layers for nodes
    max_nodes: 5                 # Maximum number of nodes
    max_poles: 4                 # Maximum number of poles
    max_zeros: 4                 # Maximum number of zeros
    dropout: 0.1                 # Dropout probability

# Training Configuration
training:
  # Two-phase training
  phase1_epochs: 100             # Freeze encoder, train decoder only
  phase2_epochs: 100             # Joint fine-tuning
  total_epochs: 200              # Total training epochs

  batch_size: 4                  # Batch size
  learning_rate_phase1: 1.0e-4   # Learning rate for phase 1 (decoder only)
  learning_rate_phase2: 5.0e-5   # Learning rate for phase 2 (joint)

  weight_decay: 1.0e-5           # L2 regularization
  grad_clip: 1.0                 # Gradient clipping (standard, no need for tight clipping)

  kl_weight: 0.01                # KL divergence weight (phase 2 only)

# Loss Weights
loss:
  # All losses weighted equally (NO 50x needed!)
  node_type_weight: 1.0          # Node type prediction
  edge_exist_weight: 1.0         # Edge existence prediction (autoregressive handles imbalance!)
  edge_value_weight: 1.0         # Edge value prediction
  pole_count_weight: 1.0         # Pole count prediction
  zero_count_weight: 1.0         # Zero count prediction
  pole_value_weight: 1.0         # Pole value prediction
  zero_value_weight: 1.0         # Zero value prediction

  # Optional transfer function loss
  tf_weight: 0.0                 # Set to 0.5 if you want to add TF loss

# Edge Generation Monitoring
edge_monitoring:
  enabled: true                  # Enable edge generation validation
  warning_threshold: 0.3         # Warn if mean edge prob < this (higher than diffusion)
  critical_threshold: 0.1        # Critical alert if mean edge prob < this
  check_every_n_batches: 10      # Check edge stats every N batches

# Data Configuration
data:
  dataset_path: 'rlc_dataset/filter_dataset.pkl'
  train_split: 0.8               # 80% training, 20% validation

# Checkpoint Configuration
checkpoint:
  pretrained_encoder: 'checkpoints/variable_length/20251222_102121/best.pt'  # Pretrained encoder
  save_dir: 'checkpoints/graphgpt_decoder'
  save_frequency: 10             # Save checkpoint every N epochs
  keep_best: true                # Keep best validation loss checkpoint

# Validation Configuration
validation:
  frequency: 1                   # Validate every N epochs
  num_samples: 10                # Number of samples to generate for visualization

# Logging Configuration
logging:
  log_frequency: 10              # Log metrics every N batches
  tensorboard: false             # Use TensorBoard logging
  verbose: true                  # Print detailed metrics
